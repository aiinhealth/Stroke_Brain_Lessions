{"cells":[{"cell_type":"markdown","metadata":{"id":"v2XvrOWkyk9O"},"source":["First create a direct access to /datasets folder in your personal drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32175,"status":"ok","timestamp":1684684932975,"user":{"displayName":"Alejandro M","userId":"11111187776730726759"},"user_tz":300},"id":"eMNWO08O81RZ","outputId":"3c2f9eaf-c0f3-48fc-d661-534514db257d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount drive if needed\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"v8rt_QUUthFp"},"source":["\n","## Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36322,"status":"ok","timestamp":1684617559554,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"Bdm-G9y_1Wrw","outputId":"a7326027-fe6c-4c44-9943-ebf562e3424d"},"outputs":[],"source":["! pip install SimpleITK\n","! pip install antspyx"]},{"cell_type":"markdown","metadata":{"id":"4B0BhRTnxlxj"},"source":["\n","## Load images to current session"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7979,"status":"ok","timestamp":1684617590683,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"GhAcoJXWxqgK","outputId":"51d4abdf-e2c6-407a-c4b6-10ae91b99e4a"},"outputs":[],"source":["! mkdir -v data\n","! unzip \"/content/drive/MyDrive/integradora_fiec/datasets/NATIVE_FILTERED_MANUALLY.zip\" -d \"/data\""]},{"cell_type":"markdown","metadata":{"id":"9nUkyINb3zCR"},"source":["## Preprocessing steps functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4712,"status":"ok","timestamp":1684617606998,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"0rHZzByP5XeA","outputId":"01058e50-e5ce-4036-d881-e56a418cbe43"},"outputs":[{"name":"stdout","output_type":"stream","text":["AntsPy version = 0.3.8\n","SimpleITK version = 2.2.1\n"]}],"source":["%matplotlib inline\n","import os\n","import ants\n","import SimpleITK as sitk\n","\n","print(f'AntsPy version = {ants.__version__}')\n","print(f'SimpleITK version = {sitk.__version__}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"id8_NUqU4iHV"},"outputs":[],"source":["mni_T1_path = TEMPLATE_PATH = '/content/drive/MyDrive/integradora_fiec/datasets/templates/mni_icbm152_t1_tal_nlin_sym_09a.nii'\n","\n","def load_template_ants() -> ants.ANTsImage:\n","    \"\"\"\n","    Loads the template image using the ANTs library.\n","\n","    Returns:\n","        ants.ANTsImage: The loaded template image as an ANTsImage object.\n","    \"\"\"\n","    template_img_ants = ants.image_read(TEMPLATE_PATH)\n","    return template_img_ants\n","\n","def load_img_ants(path: str) -> ants.ANTsImage:\n","    \"\"\"\n","    Loads an image from the specified file path using the ANTs library.\n","\n","    Args:\n","        path (str): The file path of the image to be loaded.\n","\n","    Returns:\n","        ants.ANTsImage: The loaded image as an ANTsImage object.\n","    \"\"\"\n","    raw_img_ants = ants.image_read(path)\n","    return raw_img_ants\n","\n","def register_to_mni(img: ants.ANTsImage, mask: ants.ANTsImage) -> ants.ANTsImage:\n","    \"\"\"\n","    Registers an MRI image and its associated mask to the MNI space using ANTs library.\n","\n","    Args:\n","        img (ants.ANTsImage): The MRI image to be registered.\n","        mask (ants.ANTsImage): The mask associated with the MRI image.\n","\n","    Returns:\n","        ants.ANTsImage: The registered MRI image in MNI space.\n","        ants.ANTsImage: The registered mask in MNI space.\n","    \"\"\"\n","    template_img = load_template_ants()\n","    transformation = ants.registration(fixed=template_img, moving=img, type_of_transform='SyN')\n","\n","    img_registered = transformation['warpedmovout']\n","  \n","    mask_registered = ants.apply_transforms(fixed=template_img,moving=mask,transformlist=transformation['fwdtransforms'])\n","    return img_registered, mask_registered"]},{"cell_type":"markdown","metadata":{"id":"x3nnbYgj4qnZ"},"source":["## Register"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r_861Ot4svp"},"outputs":[],"source":["from glob import glob\n","\n","xpaths = sorted(glob(f'/data/NATIVE_FILTERED_MANUALLY/train/*/*/*01.nii.gz') )\n","ypaths = sorted(glob(f'/data/NATIVE_FILTERED_MANUALLY/train/*/*/*_LesionSmooth.nii.gz'))\n","assert len(xpaths) == len(ypaths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684617626409,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"2_L0ZjyRMYer","outputId":"9927f6bc-572e-499d-ab22-1477ef7dc05c"},"outputs":[],"source":["print(\"Number of samples:\", len(xpaths))\n","for input_path, target_path in zip(xpaths, ypaths):\n","    print(input_path[-35:], \"|\", target_path[-48:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2823194,"status":"ok","timestamp":1684620459247,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"eCWkTx3B5vfL","outputId":"3a1e6484-32d9-4b5b-8b4a-2a8760d2c251"},"outputs":[],"source":["# Loop over the pairs of input and target file paths and perform registration to MNI space\n","for i,(xpath, ypath) in enumerate(zip(xpaths, ypaths)):\n","  folder = xpath[:-20]\n","  file_name = xpath[:-7][-13:]\n","  x_registered_path = folder + file_name + '_registered.nii.gz'\n","  y_registered_path = folder + file_name + '_LesionSmooth_registered.nii.gz'\n","\n","  x3d = load_img_ants(xpath)\n","  y3d = load_img_ants(ypath)\n","\n","  x3d_registered, y3d_registered = register_to_mni(img=x3d,mask=y3d)\n","\n","  print(i, x_registered_path)\n","  print(i, y_registered_path)\n","\n","  x3d_registered.to_file(x_registered_path)\n","  y3d_registered.to_file(y_registered_path)\n","\n","  #if i == 0 : break\n"]},{"cell_type":"markdown","metadata":{"id":"lmgC4KnjDc47"},"source":["## Bias Field Correction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RLqPRCMDgnv"},"outputs":[],"source":["xpaths = sorted(glob(f'/data/NATIVE_FILTERED_MANUALLY/train/*/*/*01_registered.nii.gz') )\n","ypaths = sorted(glob(f'/data/NATIVE_FILTERED_MANUALLY/train/*/*/*_LesionSmooth_registered.nii.gz'))\n","assert len(xpaths) == len(ypaths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1684622758350,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"pL92X1T0DaUQ","outputId":"e2045b66-5d03-4d9f-8727-e060e9072823"},"outputs":[],"source":["print(\"Number of samples:\", len(xpaths))\n","for input_path, target_path in zip(xpaths, ypaths):\n","    print(input_path[-35:], \"|\", target_path[-48:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkXbZBb-Do1b"},"outputs":[],"source":["def bias_field_correction(img: sitk.Image) -> sitk.Image:\n","    \"\"\"\n","    Perform bias field correction on the input image using N4BiasFieldCorrection.\n","\n","    Args:\n","        img (sitk.Image): The input image to be bias corrected.\n","\n","    Returns:\n","        sitk.Image: The bias-corrected image.\n","    \"\"\"\n","    head_mask = sitk.RescaleIntensity(img, 0, 255)\n","    head_mask = sitk.LiThreshold(head_mask,0,1)\n","\n","    shrinkFactor = 4\n","    inputImage = img\n","    inputImage = sitk.Shrink( img, [ shrinkFactor ] * inputImage.GetDimension() )\n","    maskImage = sitk.Shrink( head_mask, [ shrinkFactor ] * inputImage.GetDimension() )\n","\n","    bias_corrector = sitk.N4BiasFieldCorrectionImageFilter()\n","    bias_corrector.Execute(inputImage, maskImage)\n","\n","    log_bias_field = bias_corrector.GetLogBiasFieldAsImage(img)\n","    result = img / sitk.Exp( log_bias_field ) # corrected img at full resolution\n","\n","    # output of division has 64 pixel type, we cast it to float32 to keep compatibility\n","    result = sitk.Cast(result, sitk.sitkFloat32)\n","    \n","    return result\n","\n","def load_img_sitk(path: str) -> sitk.Image:\n","    \"\"\"\n","    Load an image using SimpleITK (sitk) and return it.\n","\n","    Args:\n","        path (str): The path to the image file.\n","\n","    Returns:\n","        sitk.Image: The loaded image as a SimpleITK Image object.\n","    \"\"\"\n","    raw_img_sitk = sitk.ReadImage(path, sitk.sitkFloat32)\n","    return raw_img_sitk\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306882,"status":"ok","timestamp":1684623365527,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"Ye88JcOcEPIi","outputId":"1d829387-ac69-4edb-d08a-6f14e75de4a9"},"outputs":[],"source":["for i,(xpath, ypath) in enumerate(zip(xpaths, ypaths)):\n","  # Extract folder and file name information from the input and target file paths\n","  folder = xpath[:-20]\n","  file_name = xpath[:-7][-13:]\n","  # Create the output path for the bias field corrected image\n","  x_out_path = folder + file_name + '_BF.nii.gz'\n","\n","  x3d = load_img_sitk(xpath)\n","  x3d_bf_corrected = bias_field_correction(x3d)\n","\n","  sitk.WriteImage(x3d_bf_corrected, x_out_path)\n","  # Print the progress (index) and the output path of the bias field corrected image\n","  print(i, x_out_path)\n","\n","  #if i == 0 : break\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f-m0hqWbFmyb"},"source":["## Prepare training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nn2SgDqQXTLr"},"outputs":[],"source":["xpaths = sorted(glob(f'/data/NATIVE_FILTERED_MANUALLY/train/*/*/*01_registered_BF.nii.gz') )\n","ypaths = sorted(glob(f'/data/NATIVE_FILTERED_MANUALLY/train/*/*/*_LesionSmooth_registered.nii.gz'))\n","assert len(xpaths) == len(ypaths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684623378122,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"SBlYCZJEXVir","outputId":"4cb5a127-2f21-435f-da32-1501874ce1b0"},"outputs":[],"source":["print(\"Number of samples:\", len(xpaths))\n","for input_path, target_path in zip(xpaths, ypaths):\n","    print(input_path[-35:], \"|\", target_path[-48:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uEzKrrwFpHg"},"outputs":[],"source":["# load mni152 brain mask\n","TEMPLATE_BRAIN_MASK_PATH = '/content/drive/MyDrive/integradora_fiec/datasets/templates/mni_icbm152_t1_tal_nlin_sym_09a_mask.nii'\n","mni152_brain_mask = sitk.ReadImage(TEMPLATE_BRAIN_MASK_PATH, sitk.sitkFloat32)\n","mni152_T1 = sitk.ReadImage(TEMPLATE_PATH, sitk.sitkFloat32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RiCu-67XB7V"},"outputs":[],"source":["def preprocess_ximg(ximg: sitk.Image, flipped = False) -> np.ndarray:\n","  \"\"\"\n","    Preprocess the input image (ximg) using several SimpleITK image processing operations.\n","    \n","    Args:\n","        ximg (sitk.Image): The input image in SimpleITK format.\n","        flipped (bool, optional): Flag to determine whether to flip the image or not.\n","            Defaults to False.\n","\n","    Returns:\n","        np.ndarray: The preprocessed 3D numpy array representing the image.\n","\n","    \"\"\"\n","  x3d = sitk.HistogramMatching(ximg, mni152_T1)\n","  x3d = sitk.Multiply(x3d, mni152_brain_mask) # mask brain\n","  x3d = sitk.CurvatureAnisotropicDiffusion(x3d, conductanceParameter=1, numberOfIterations=1) # denoise a bit\n","  \n","  if flipped:\n","    x3d = sitk.Flip(x3d,(True, False, False))\n","  \n","  x3d = sitk.GetArrayFromImage(x3d)\n","  x3d = x3d[30:160,4:228,14:190] # crop to size -> (130, 224, 176)\n","  x3d = x3d / 255.0\n","  x3d = np.expand_dims(x3d,3) # add channel -> (130, 224, 176, 1)\n","  assert x3d.shape == (130,224,176,1)\n","  return x3d\n","\n","def preprocess_yimg(yimg: sitk.Image, flipped=False) -> np.ndarray:\n","  \"\"\"\n","    Preprocess the target image (yimg) for segmentation using SimpleITK image processing operations.\n","    \n","    Args:\n","        yimg (sitk.Image): The target image in SimpleITK format.\n","        flipped (bool, optional): Flag to determine whether to flip the image or not.\n","            Defaults to False.\n","\n","    Returns:\n","        np.ndarray: The preprocessed 3D numpy array representing the target segmentation.\n","\n","    \"\"\"\n","  y3d = yimg\n","\n","  if flipped:\n","    y3d = sitk.Flip(y3d,(True, False, False))\n","  \n","  y3d = sitk.GetArrayFromImage(y3d)\n","  y3d = y3d[30:160,4:228,14:190] # crop to size -> (130, 224, 176)\n","  y3d = y3d / 255.0\n","  y3d = np.expand_dims(y3d,3) # add channel -> (130, 224, 176, 1)\n","  assert x3d.shape == (130,224,176,1)\n","  return y3d\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132088,"status":"ok","timestamp":1684623530636,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"BrCYA5cgXPNr","outputId":"1ff1fe26-53f3-40f6-89a3-927441ce9109"},"outputs":[{"name":"stdout","output_type":"stream","text":["...................."]}],"source":["ROW_SIZE = 224 # Height of the model input\n","COL_SIZE = 176 # Width of the model input\n","\n","X = np.empty((0,ROW_SIZE,COL_SIZE,1), dtype=np.float32) # Placeholder for preprocessed input images\n","Y = np.empty((0,ROW_SIZE,COL_SIZE,1), dtype=np.float32) # Placeholder for preprocessed label images\n","\n","for i,(xpath, ypath) in enumerate(zip(xpaths, ypaths)):\n","\n","    ximg        =   sitk.ReadImage(xpath, sitk.sitkFloat32)\n","    x3d         =  preprocess_ximg(ximg) \n","    flipped_x3d =  preprocess_ximg(ximg, flipped=True)\n","\n","    yimg        =   sitk.ReadImage(ypath, sitk.sitkFloat32)\n","    y3d         =  preprocess_yimg(yimg) \n","    flipped_y3d =  preprocess_yimg(yimg, flipped=True)\n","\n","    # Concatenate the preprocessed images (original and flipped) along the first axis (number of samples)\n","    x3d = np.concatenate((x3d, flipped_x3d), axis=0)\n","    y3d = np.concatenate((y3d, flipped_y3d), axis=0)\n","\n","    # Ensure the shapes of the concatenated arrays are as expected\n","    assert x3d.shape  == (260,224,176, 1)\n","    assert y3d.shape  == (260,224,176, 1)\n","\n","    X = np.concatenate((X, x3d), axis=0)\n","    Y = np.concatenate((Y, y3d), axis=0)\n","\n","    print('.', end='')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1684623536305,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"hDweE5HsXc2z","outputId":"eea054e4-1c0b-4206-cc47-25fa6ddca341"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5200, 224, 176, 1) (5200, 224, 176, 1)\n"]}],"source":["print(X.shape, Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1684623678692,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"WmpXKtSKZNQI","outputId":"64d275b4-aefe-4abf-ade0-ffdf169dae9d"},"outputs":[{"data":{"text/plain":["(5200, 224, 176)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["X[:,:,:,0].shape"]},{"cell_type":"markdown","metadata":{"id":"ltHhwU2sY0TD"},"source":["## Double check slices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCC2IlRXbUpL"},"outputs":[],"source":["def get_x2d_marked(x2d,y2d):\n","  \"\"\"\n","    Mark the input 2D image (x2d) using the contours of the label image (y2d).\n","\n","    Args:\n","        x2d (np.ndarray): The 2D input image.\n","        y2d (np.ndarray): The 2D label image.\n","\n","    Returns:\n","        np.ndarray: The marked version of the input image.\n","    \"\"\"\n","  dilation_level = 4\n","  m = (y2d).astype('uint8')\n","  m = sitk.GetImageFromArray(m)\n","  m = sitk.BinaryDilate(m,(dilation_level,1,1))\n","  m = sitk.BinaryContour(m)\n","\n","  x2d_marked = sitk.GetImageFromArray(x2d)\n","  x2d_marked = sitk.MaskNegated(x2d_marked, sitk.Cast(m,sitk.sitkFloat32))\n","  x2d_marked = sitk.GetArrayFromImage(x2d_marked)\n","  return x2d_marked\n","\n","def show_slices(slices: list[np.ndarray], cmap: str ='gray'):\n","  \"\"\"\n","    Display a list of image slices (2D arrays) as subplots in a single figure.\n","\n","    Args:\n","        slices (list[np.ndarray]): A list of 2D numpy arrays representing image slices.\n","        cmap (str, optional): The colormap to be used for visualization. Defaults to 'gray'.\n","    \"\"\"\n","  fig, axes = plt.subplots(len(slices), 1, figsize=(15,15))\n","  for i, slice in enumerate(slices):\n","    axes[i].imshow(slice, cmap=cmap)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1JBRG54fzeib_ox4P_O5BrYzHKyWzK4qv"},"executionInfo":{"elapsed":8971,"status":"ok","timestamp":1684624259410,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"eZaRB9ZobXN2","outputId":"98f104b6-d690-4f29-c072-4ec9c595fd48"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["STEPS = 150\n","c=0\n","for i in range(0,len(X),STEPS):\n","  x, y = X[i], Y[i]\n","  if len(np.unique(y)) == 1:\n","    continue\n","  x2d_marked = get_x2d_marked(x[:,:,0],y[:,:,0])\n","  show_slices([x2d_marked,x[:,:,0]])\n","  c+=1\n","  if c==10:\n","    break"]},{"cell_type":"markdown","metadata":{"id":"yFrpmprabxsB"},"source":["## Save training dataset as npy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAPpbgVZbreV"},"outputs":[],"source":["from numpy import save\n","# contains data processed from 20 native ATLAS imgs trough: register to mni, bias field, histogram matching, brain extraction, denoise\n","X_output_path = '/content/drive/MyDrive/integradora_fiec/datasets/paper lesions extended/dataset_clinet_input_processed_X.npy'\n","Y_output_path = '/content/drive/MyDrive/integradora_fiec/datasets/paper lesions extended/dataset_clinet_input_processed_Y.npy'\n","\n","\n","save(X_output_path, X)\n","save(Y_output_path, Y)"]},{"cell_type":"markdown","metadata":{"id":"jrrVAd_tdnYB"},"source":["## Load train set [JUMP HERE IF DATA AVAILABLE]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkMS-YnjdpD2"},"outputs":[],"source":["from numpy import load\n","X_input_path = '/content/drive/MyDrive/integradora_fiec/datasets/paper lesions extended/dataset_clinet_input_processed_X.npy'\n","Y_input_path = '/content/drive/MyDrive/integradora_fiec/datasets/paper_lesions extended/dataset_clinet_input_processed_Y.npy'\n","\n","X = load(X_input_path)\n","Y = load(Y_input_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1684625616949,"user":{"displayName":"Roberto Alejandro Mena Ramirez","userId":"17762559014980159599"},"user_tz":300},"id":"iMQcUwCQd6mi","outputId":"366562a2-8633-4060-c463-f0ba598b4b8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5200, 224, 176, 1) (5200, 224, 176, 1)\n"]}],"source":["print(X.shape, Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1816,"status":"ok","timestamp":1684685371862,"user":{"displayName":"Alejandro M","userId":"11111187776730726759"},"user_tz":300},"id":"HYCPtTeJf9qC","outputId":"1d98ef2b-99d0-4e5b-8e5a-bffa69298932"},"outputs":[{"name":"stdout","output_type":"stream","text":["(4160, 224, 176, 1) (4160, 224, 176, 1)\n","(1040, 224, 176, 1) (1040, 224, 176, 1)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)\n","print(X_train.shape, y_train.shape)\n","print(X_valid.shape, y_valid.shape)"]},{"cell_type":"markdown","metadata":{"id":"ppzbluM7fFF7"},"source":["## Define Train Model (CLCI net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWEZusUufI-b"},"outputs":[],"source":["from keras import *\n","from keras.layers import *\n","import tensorflow as tf\n","kernel_regularizer = regularizers.l2(1e-5)\n","bias_regularizer = regularizers.l2(1e-5)\n","kernel_regularizer = None\n","bias_regularizer = None\n","\n","def conv_lstm(input1, input2, channel=256):\n","    \"\"\"\n","    Creates a ConvLSTM2D layer by combining two input tensors.\n","\n","    This function reshapes the input tensors to add a time dimension and then concatenates them along the time\n","    dimension. It then applies a ConvLSTM2D layer with specified channel size, kernel size, strides, and\n","    optional regularization.\n","\n","    Args:\n","        input1 (tf.Tensor): The first input tensor to the ConvLSTM layer.\n","        input2 (tf.Tensor): The second input tensor to the ConvLSTM layer.\n","        channel (int): The number of output channels (filters) of the ConvLSTM layer. Default is 256.\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the ConvLSTM layer.\n","\n","    \"\"\"\n","    lstm_input1 = Reshape((1, input1.shape.as_list()[1], input1.shape.as_list()[2], input1.shape.as_list()[3]))(input1)\n","    lstm_input2 = Reshape((1, input2.shape.as_list()[1], input2.shape.as_list()[2], input1.shape.as_list()[3]))(input2)\n","\n","    lstm_input = custom_concat(axis=1)([lstm_input1, lstm_input2])\n","    x = ConvLSTM2D(channel, (3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal', kernel_regularizer=kernel_regularizer)(lstm_input)\n","    return x\n","\n","def conv_2(inputs, filter_num, kernel_size=(3,3), strides=(1,1), kernel_initializer='glorot_uniform', kernel_regularizer = kernel_regularizer):\n","    \"\"\"\n","    Defines a 2D Convolution block with optional regularization.\n","\n","    This function creates a 2D convolutional block consisting of two Conv2D layers with batch normalization\n","    and ReLU activation functions. It allows for specifying the number of filters (channels), kernel size,\n","    strides, kernel initializer, and an optional regularization.\n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional block.\n","        filter_num (int): The number of filters (channels) in the convolutional layers.\n","        kernel_size (tuple): The size of the convolutional kernel, specified as a tuple of two\n","                             integers representing the height and width of the kernel, respectively.\n","                             Default is (3, 3).\n","        strides (tuple): The strides of the convolution along the height and width dimensions,\n","                         specified as a tuple of two integers. Default is (1, 1).\n","        kernel_initializer (str): The initializer for the convolutional kernels. Options include 'glorot_uniform'\n","                                  and 'he_normal'. Default is 'glorot_uniform'.\n","        kernel_regularizer (tf.keras.regularizers.Regularizer or None): An optional regularization applied\n","                                                                       to the convolutional kernel. Default is None.\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the convolutional block.\n","\n","    \"\"\"\n","    conv_ = Conv2D(filter_num, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=kernel_initializer, kernel_regularizer = kernel_regularizer)(inputs)\n","    conv_ = BatchNormalization()(conv_)\n","    conv_ = Activation('relu')(conv_)\n","    conv_ = Conv2D(filter_num, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=kernel_initializer, kernel_regularizer = kernel_regularizer)(conv_)\n","    conv_ = BatchNormalization()(conv_)\n","    conv_ = Activation('relu')(conv_)   \n","    return conv_\n","\n","def conv_2_init(inputs, filter_num, kernel_size=(3,3), strides=(1,1)):\n","    \"\"\"\n","    Defines a 2D Convolution block with 'he_normal' kernel initializer.\n","\n","    This function creates a 2D convolutional layer with a specified number of filters (channels),\n","    kernel size, strides, and 'he_normal' kernel initializer.\n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional layer.\n","        filter_num (int): The number of filters (channels) in the convolutional layer.\n","        kernel_size (tuple): The size of the convolutional kernel, specified as a tuple of two\n","                             integers representing the height and width of the kernel, respectively.\n","                             Default is (3, 3).\n","        strides (tuple): The strides of the convolution along the height and width dimensions,\n","                         specified as a tuple of two integers. Default is (1, 1).\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the convolutional block.\n","\n","    \"\"\"\n","    return conv_2(inputs, filter_num, kernel_size=kernel_size, strides=strides, kernel_initializer='he_normal', kernel_regularizer = kernel_regularizer) \n","\n","def conv_2_init_regularization(inputs, filter_num, kernel_size=(3,3), strides=(1,1)):\n","    \"\"\"\n","    Defines a 2D Convolution block \n","\n","    This function creates a 2D convolutional layer with a specified number of filters (channels),\n","    kernel size, strides, 'he_normal' kernel initializer, and L2 regularization with a weight decay\n","    of 5e-4.\n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional layer.\n","        filter_num (int): The number of filters (channels) in the convolutional layer.\n","        kernel_size (tuple): The size of the convolutional kernel, specified as a tuple of two\n","                             integers representing the height and width of the kernel, respectively.\n","                             Default is (3, 3).\n","        strides (tuple): The strides of the convolution along the height and width dimensions,\n","                         specified as a tuple of two integers. Default is (1, 1).\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the convolutional block.\n","\n","    \"\"\"\n","    return conv_2(inputs, filter_num, kernel_size=kernel_size, strides=strides, kernel_initializer='he_normal', kernel_regularizer = regularizers.l2(5e-4)) \n","\n","def conv_1(inputs, filter_num, kernel_size=(3,3), strides=(1,1), kernel_initializer='glorot_uniform', kernel_regularizer = kernel_regularizer):\n","    \"\"\"\n","    Defines a 2D Convolution block\n","\n","    This function creates a 2D convolutional layer with a specified number of filters (channels),\n","    kernel size, strides, kernel initializer, and optional regularization.\n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional layer.\n","        filter_num (int): The number of filters (channels) in the convolutional layer.\n","        kernel_size (tuple): The size of the convolutional kernel, specified as a tuple of two\n","                             integers representing the height and width of the kernel, respectively.\n","                             Default is (3, 3).\n","        strides (tuple): The strides of the convolution along the height and width dimensions,\n","                         specified as a tuple of two integers. Default is (1, 1).\n","        kernel_initializer (str): The kernel initializer for the convolutional layer. It can be either\n","                                   'glorot_uniform' or 'he_normal' or any other valid initializer\n","                                   available in Keras. Default is 'glorot_uniform'.\n","        kernel_regularizer (tf.keras.regularizers.Regularizer or None): Optional regularization to be\n","                                                                       applied to the kernel weights of\n","                                                                       the convolutional layer.\n","                                                                       Default is None.\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the convolutional block.\n","    \"\"\"\n","    conv_ = Conv2D(filter_num, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer=kernel_initializer, kernel_regularizer = kernel_regularizer)(inputs)\n","    conv_ = BatchNormalization()(conv_)\n","    conv_ = Activation('relu')(conv_)\n","    return conv_\n","\n","def conv_1_init(inputs, filter_num, kernel_size=(3,3), strides=(1,1)):\n","    \"\"\"\n","    Defines a 2D Convolution block\n","\n","    This function creates a 2D convolutional layer with a specified number of filters (channels),\n","    kernel size, strides, and 'he_normal' kernel initializer. Additionally, it allows for optional\n","    regularization by providing the `kernel_regularizer` parameter, which can be set to None if no\n","    regularization is desired.\n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional layer.\n","        filter_num (int): The number of filters (channels) in the convolutional layer.\n","        kernel_size (tuple): The size of the convolutional kernel, specified as a tuple of two\n","                             integers representing the height and width of the kernel respectively.\n","                             Default is (3, 3).\n","        strides (tuple): The strides of the convolution along the height and width dimensions,\n","                         specified as a tuple of two integers. Default is (1, 1).\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the convolutional block.\n","\n","    \"\"\"\n","    return conv_1(inputs, filter_num, kernel_size=kernel_size, strides=strides, kernel_initializer='he_normal', kernel_regularizer = kernel_regularizer) \n","\n","def conv_1_init_regularization(inputs, filter_num, kernel_size=(3,3), strides=(1,1)):\n","    \"\"\"\n","    Creates a 2D Convolution block\n","\n","    This function defines a 2D convolutional layer with a specified number of filters (channels),\n","    kernel size, strides, 'he_normal' kernel initializer, and fixed L2 regularization with a\n","    regularization strength of 5e-4.\n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional layer.\n","        filter_num (int): The number of filters (channels) in the convolutional layer.\n","        kernel_size (tuple): The size of the convolutional kernel, specified as a tuple of two\n","                             integers representing the height and width of the kernel respectively.\n","                             Default is (3, 3).\n","        strides (tuple): The strides of the convolution along the height and width dimensions,\n","                         specified as a tuple of two integers. Default is (1, 1).\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the convolutional block.\n","\n","    \"\"\"\n","    return conv_1(inputs, filter_num, kernel_size=kernel_size, strides=strides, kernel_initializer='he_normal', kernel_regularizer = regularizers.l2(5e-4))\n","\n","def dilate_conv(inputs, filter_num, dilation_rate):\n","    \"\"\"\n","    Creates a dilated Conv2D layer.\n","\n","    This function defines a 2D convolutional layer with a specified number of filters and\n","    dilation rate. \n","\n","    Args:\n","        inputs (tf.Tensor): The input tensor to the convolutional layer.\n","        filter_num (int): The number of filters (channels) in the convolutional layer.\n","        dilation_rate (tuple): The dilation rate for the convolutional layer, specified\n","                               as a tuple of two integers representing the dilation rate\n","                               along the height and width dimensions respectively.\n","\n","    Returns:\n","        tf.Tensor: The output tensor of the dilated convolutional layer.\n","\n","    \"\"\"\n","    conv_ = Conv2D(filter_num, kernel_size=(3,3), dilation_rate=dilation_rate, padding='same', kernel_initializer='he_normal', kernel_regularizer = kernel_regularizer)(inputs)\n","    conv_ = BatchNormalization()(conv_)\n","    conv_ = Activation('relu')(conv_)\n","    return conv_\n","\n","class custom_concat(Layer):\n","    \"\"\"\n","    Custom Keras layer to perform concatenation along a specified axis.\n","\n","    This layer takes multiple input tensors and concatenates them along the specified axis.\n","    The concatenation is performed element-wise along the axis, preserving the dimensions\n","    of all other axes.\n","\n","    Args:\n","        axis (int): The axis along which to concatenate the inputs. The default value is -1,\n","                    which corresponds to the last axis.\n","\n","    Attributes:\n","        axis (int): The axis along which the concatenation is performed.\n","\n","    \"\"\"\n","    def __init__(self, axis=-1, **kwargs):\n","        \"\"\"\n","        Initializes the custom_concat layer.\n","\n","        Args:\n","            axis (int): The axis along which to concatenate the inputs.\n","\n","        \"\"\"\n","        super(custom_concat, self).__init__(**kwargs)\n","        self.axis = axis\n","\n","    def build(self, input_shape):\n","        \"\"\"\n","        Builds the custom_concat layer.\n","\n","        Args:\n","            input_shape (tuple): The shape of the input tensor(s).\n","\n","        \"\"\"\n","        # Create a trainable weight variable for this layer.\n","        self.built = True\n","        super(custom_concat, self).build(input_shape)  # Be sure to call this somewhere!\n","\n","    def call(self, x):\n","        \"\"\"\n","        Performs the concatenation operation on the input tensors.\n","\n","        Args:\n","            x (list): A list of input tensors to be concatenated.\n","\n","        Returns:\n","            tf.Tensor: The concatenated tensor.\n","\n","        \"\"\"\n","        self.res = tf.concat(x, self.axis)\n","\n","        return self.res\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\"\n","        Computes the output shape of the custom_concat layer.\n","\n","        Args:\n","            input_shape (tuple): The shape of the input tensor(s).\n","\n","        Returns:\n","            tuple: The shape of the concatenated output tensor.\n","\n","        \"\"\"\n","        input_shapes = input_shape\n","        output_shape = list(input_shapes[0])\n","\n","        for shape in input_shapes[1:]:\n","            if output_shape[self.axis] is None or shape[self.axis] is None:\n","                output_shape[self.axis] = None\n","                break\n","            output_shape[self.axis] += shape[self.axis]\n","\n","        return tuple(output_shape)\n","\n","\n","class BilinearUpsampling(Layer):\n","    def __init__(self, upsampling=(2, 2), **kwargs):\n","        \"\"\"\n","        Initializes the BilinearUpsampling layer.\n","\n","        Args:\n","            upsampling: A tuple specifying the upsampling factor along the height and width dimensions.\n","                       Default is (2, 2), i.e., upsampling by a factor of 2 in both height and width.\n","            **kwargs: Additional keyword arguments to pass to the base class (Layer).\n","\n","        \"\"\"\n","        super(BilinearUpsampling, self).__init__(**kwargs)       \n","        self.upsampling = upsampling\n","        \n","    def compute_output_shape(self, input_shape):\n","        \"\"\"\n","        Computes the output shape of the layer based on the input shape.\n","\n","        Args:\n","            input_shape: A tuple representing the input shape (batch_size, height, width, channels).\n","\n","        Returns:\n","            Tuple representing the output shape (batch_size, new_height, new_width, channels)\n","            after applying the upsampling factor.\n","\n","        \"\"\"\n","        height = self.upsampling[0] * \\\n","                 input_shape[1] if input_shape[1] is not None else None\n","        width = self.upsampling[1] * \\\n","                input_shape[2] if input_shape[2] is not None else None\n","        return (input_shape[0],\n","                height,\n","                width,\n","                input_shape[3])\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        Performs the upsampling operation using bilinear interpolation.\n","\n","        Args:\n","            inputs: The input tensor (batch_size, height, width, channels).\n","\n","        Returns:\n","            The upscaled tensor obtained using bilinear interpolation.\n","\n","        \"\"\"\n","        #return tf.image.resize_bilinear(inputs, (int(inputs.shape[1] * self.upsampling[0]),\n","        #                                           int(inputs.shape[2] * self.upsampling[1])))\n","        return tf.image.resize(inputs, (int(inputs.shape[1] * self.upsampling[0]),\n","                                                   int(inputs.shape[2] * self.upsampling[1])))\n","\n","\n","\n","def concat_pool(conv, pool, filter_num, strides=(2, 2)):\n","    \"\"\"\n","    Concatenates a Convolutional layer with a Pooling layer.\n","\n","    Args:\n","        conv: Input Convolutional layer.\n","        pool: Input Pooling layer.\n","        filter_num: Number of filters for the Convolutional layer.\n","        strides: Strides for the Convolutional layer. Default is (2, 2).\n","\n","    Returns:\n","        A concatenated layer obtained by concatenating the Convolutional layer and the Pooling layer.\n","    \"\"\"\n","    conv_downsample = Conv2D(filter_num, (3, 3), strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=kernel_regularizer)(conv)\n","    conv_downsample = BatchNormalization()(conv_downsample)\n","    conv_downsample = Activation('relu')(conv_downsample)\n","    concat_pool_ = Concatenate()([conv_downsample, pool])\n","    return concat_pool_\n","######################################\n","from keras.optimizers import Adam\n","import keras.backend as K\n","#from custom_layer import *\n","\n","\n","def dice_coef(y_true, y_pred):\n","    \"\"\"\n","    This function calculates the Dice coefficient, which is a metric commonly used in image segmentation tasks\n","    to evaluate the similarity between the predicted segmentation and the ground truth.\n","\n","    Args:\n","        y_true (tf.Tensor): The ground truth segmentation mask.\n","        y_pred (tf.Tensor): The predicted segmentation mask.\n","\n","    Returns:\n","        tf.Tensor: The Dice coefficient.\n","    \"\"\"\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + 1) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + 1)\n","\n","def dice_coef_loss(y_true, y_pred):\n","    \"\"\"\n","    Dice Coefficient Loss Function.\n","\n","    Args:\n","        y_true (tf.Tensor): The ground truth segmentation mask.\n","        y_pred (tf.Tensor): The predicted segmentation mask.\n","\n","    Returns:\n","        tf.Tensor: The Dice coefficient loss.\n","    \"\"\"\n","    return 1. - dice_coef(y_true, y_pred)\n","\n","def CLCI_Net(input_shape=(224, 176, 1), num_class=1):\n","    \"\"\"\n","    Creates the CLCI_Net model for semantic segmentation.\n","\n","    Args:\n","        input_shape: Tuple representing the shape of the input tensor (height, width, channels).\n","                     The row and column of the input should be resized or cropped to an integer multiple of 16.\n","        num_class: Number of classes for segmentation. For binary segmentation, num_class should be set to 1.\n","\n","    Returns:\n","        Model: Keras model representing the CLCI_Net for semantic segmentation.\n","\n","    \"\"\"\n","    \n","    # The row and col of input should be resized or cropped to an integer multiple of 16.\n","    inputs = Input(shape=input_shape)\n","\n","    conv1 = conv_2_init(inputs, 32)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    concat_pool11 = concat_pool(conv1, pool1, 32, strides=(2, 2))\n","    fusion1 = conv_1_init(concat_pool11, 64 * 4, kernel_size=(1, 1))\n","\n","    conv2 = conv_2_init(fusion1, 64)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    concat_pool12 = concat_pool(conv1, pool2, 64, strides=(4, 4))\n","    concat_pool22 = concat_pool(conv2, concat_pool12, 64, strides=(2, 2))\n","    fusion2 = conv_1_init(concat_pool22, 128 * 4, kernel_size=(1, 1))\n","\n","    conv3 = conv_2_init(fusion2, 128)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    concat_pool13 = concat_pool(conv1, pool3, 128, strides=(8, 8))\n","    concat_pool23 = concat_pool(conv2, concat_pool13, 128, strides=(4, 4))\n","    concat_pool33 = concat_pool(conv3, concat_pool23, 128, strides=(2, 2))\n","    fusion3 = conv_1_init(concat_pool33, 256 * 4, kernel_size=(1, 1))\n","\n","    conv4 = conv_2_init(fusion3, 256)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n","    concat_pool14 = concat_pool(conv1, pool4, 256, strides=(16, 16))\n","    concat_pool24 = concat_pool(conv2, concat_pool14, 256, strides=(8, 8))\n","    concat_pool34 = concat_pool(conv3, concat_pool24, 256, strides=(4, 4))\n","    concat_pool44 = concat_pool(conv4, concat_pool34, 256, strides=(2, 2))\n","    fusion4 = conv_1_init(concat_pool44, 512 * 4, kernel_size=(1, 1))\n","\n","    conv5 = conv_2_init(fusion4, 512)\n","    conv5 = Dropout(0.5)(conv5)\n","\n","    clf_aspp = CLF_ASPP(conv5, conv1, conv2, conv3, conv4, input_shape)\n","\n","    up_conv1 = UpSampling2D(size=(2, 2))(clf_aspp)\n","    up_conv1 = conv_1_init(up_conv1, 256, kernel_size=(2, 2))\n","    skip_conv4 = conv_1_init(conv4, 256, kernel_size=(1, 1))\n","    context_inference1 = conv_lstm(up_conv1, skip_conv4, channel=256)\n","    conv6 = conv_2_init(context_inference1, 256)\n","\n","    up_conv2 = UpSampling2D(size=(2, 2))(conv6)\n","    up_conv2 = conv_1_init(up_conv2, 128, kernel_size=(2, 2))\n","    skip_conv3 = conv_1_init(conv3, 128, kernel_size=(1, 1))\n","    context_inference2 = conv_lstm(up_conv2, skip_conv3, channel=128)\n","    conv7 = conv_2_init(context_inference2, 128)\n","\n","    up_conv3 = UpSampling2D(size=(2, 2))(conv7)\n","    up_conv3 = conv_1_init(up_conv3, 64, kernel_size=(2, 2))\n","    skip_conv2 = conv_1_init(conv2, 64, kernel_size=(1, 1))\n","    context_inference3 = conv_lstm(up_conv3, skip_conv2, channel=64)\n","    conv8 = conv_2_init(context_inference3, 64)\n","\n","    up_conv4 = UpSampling2D(size=(2, 2))(conv8)\n","    up_conv4 = conv_1_init(up_conv4, 32, kernel_size=(2, 2))\n","    skip_conv1 = conv_1_init(conv1, 32, kernel_size=(1, 1))\n","    context_inference4 = conv_lstm(up_conv4, skip_conv1, channel=32)\n","    conv9 = conv_2_init(context_inference4, 32)\n","\n","\n","    if num_class == 1:\n","        conv10 = Conv2D(num_class, (1, 1), activation='sigmoid')(conv9)\n","    else:\n","        conv10 = Conv2D(num_class, (1, 1), activation='softmax')(conv9)\n","\n","    model = Model(inputs=inputs, outputs=conv10)\n","\n","    return model\n","\n","\n","def CLF_ASPP(conv5, conv1, conv2, conv3, conv4, input_shape):\n","    \"\"\"\n","    Creates the ASPP (Atrous Spatial Pyramid Pooling) block.\n","\n","    Args:\n","        conv5: Convolutional layer from the 5th encoder stage.\n","        conv1: Convolutional layer from the 1st encoder stage.\n","        conv2: Convolutional layer from the 2nd encoder stage.\n","        conv3: Convolutional layer from the 3rd encoder stage.\n","        conv4: Convolutional layer from the 4th encoder stage.\n","        input_shape: Shape of the input tensor (batch_size, height, width, channels).\n","\n","    Returns:\n","        Output tensor after the ASPP block.\n","\n","    \"\"\"\n","\n","    b0 = conv_1_init(conv5, 256, (1, 1))\n","    b1 = dilate_conv(conv5, 256, dilation_rate=(2, 2))\n","    b2 = dilate_conv(conv5, 256, dilation_rate=(4, 4))\n","    b3 = dilate_conv(conv5, 256, dilation_rate=(6, 6))\n","\n","    out_shape0 = input_shape[0] // pow(2, 4)\n","    out_shape1 = input_shape[1] // pow(2, 4)\n","    b4 = AveragePooling2D(pool_size=(out_shape0, out_shape1))(conv5)\n","    b4 = conv_1_init(b4, 256, (1, 1))\n","    b4 = BilinearUpsampling((out_shape0, out_shape1))(b4)\n","\n","    clf1 = conv_1_init(conv1, 256, strides=(16, 16))\n","    clf2 = conv_1_init(conv2, 256, strides=(8, 8))\n","    clf3 = conv_1_init(conv3, 256, strides=(4, 4))\n","    clf4 = conv_1_init(conv4, 256, strides=(2, 2))\n","\n","    outs = Concatenate()([clf1, clf2, clf3, clf4, b0, b1, b2, b3, b4])\n","\n","    outs = conv_1_init(outs, 256 * 4, (1, 1))\n","    outs = Dropout(0.5)(outs)\n","\n","    return outs"]},{"cell_type":"markdown","metadata":{"id":"uxHOyn9MfWnk"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6572,"status":"ok","timestamp":1684685395321,"user":{"displayName":"Alejandro M","userId":"11111187776730726759"},"user_tz":300},"id":"2Z3BHZMLfYDM","outputId":"e7e6386e-f466-4050-f12b-cc58cf3a76b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["from keras.metrics import  Recall, Precision\n","# https://stats.stackexchange.com/questions/323154/precision-vs-recall-acceptable-limits\n","# https://www.kdnuggets.com/2016/12/4-reasons-machine-learning-model-wrong.html#:~:text=Precision%20is%20a%20measure%20of,positive%20class%20are%20actually%20true.&text=Hence%2C%20a%20situation%20of%20Low,positive%20values%20are%20never%20predicted.\n","# Pre and Post processing # https://github.com/nikhilroxtomar/UNet-Segmentation-in-Keras-TensorFlow/blob/master/unet-segmentation.ipynb\n","model = CLCI_Net()\n","#model.summary()\n","model.compile(optimizer=Adam(lr=1e-4), loss=dice_coef_loss, metrics=[dice_coef,'acc',Recall(), Precision()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07RINufyfPjc"},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","\n","checkpoint_filepath = '/content/drive/MyDrive/integradora_fiec/modelos/clcinet-native-filtered-v2-{epoch:03d}-{dice_coef:03f}-{val_dice_coef:03f}.h5'\n","# Create a ModelCheckpoint callback to save the best model weights during training\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_dice_coef',\n","    mode='max',\n","    save_best_only=True)\n","# Create a ReduceLROnPlateau callback to adjust the learning rate during training\n","reduce_lr = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.2, patience=2, min_lr=2e-6)\n","# Create a list of callbacks to be used during training\n","callbacks = [\n","    model_checkpoint_callback,\n","    reduce_lr\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyoJ7LoYf32M","outputId":"9c73b839-ed07-441c-c4a1-a70992087b98"},"outputs":[],"source":["history = model.fit(\n","      X_train, y_train,\n","      batch_size=8,\n","      epochs=60,\n","      verbose=1,\n","      callbacks=callbacks,\n","      validation_data=(X_valid,y_valid))"]},{"cell_type":"markdown","metadata":{"id":"wW2znXbQhyQf"},"source":["retrain after timeout..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHm8WG9Qh1vz"},"outputs":[],"source":["model.load_weights(\"/content/drive/MyDrive/integradora_fiec/modelos/clcinet-native-filtered-v2-029-0.821825-0.787639.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb8GWyV2ihQm","outputId":"87bca2da-d826-4982-a5f4-f47222012854"},"outputs":[],"source":["history2 = model.fit(\n","      X_train, y_train,\n","      batch_size=8,\n","      epochs=31,\n","      verbose=1,\n","      callbacks=callbacks,\n","      validation_data=(X_valid,y_valid))"]},{"cell_type":"markdown","metadata":{"id":"FZwCRz4R-7yY"},"source":["## Next steps..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjUNNbMx-9P4"},"outputs":[],"source":["# It is good, fortunately we got same val_dice_coef as prev ~ 0.84\n","# So it seems now I need to do same preprocessing for all images(or only the 20?) between lacunar and mca.\n","# Make  a dataset (.npy)\n","# extract features from this dataset (csv)\n","# evaluate models perfomance ->\n","#"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["v8rt_QUUthFp","9nUkyINb3zCR","ltHhwU2sY0TD","ppzbluM7fFF7","uxHOyn9MfWnk"],"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
